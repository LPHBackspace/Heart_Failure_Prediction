# -*- coding: utf-8 -*-
"""Cola_do_Leo_Prova.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWloHCy03Dem42tXiWIMfijK2dFllZ6d

###Preparação da Tabela
"""

import numpy as np

import pandas as pd
df_heart_failure = pd.read_csv('/content/drive/MyDrive/heart_failure/heart_failure_clinical_records_dataset.csv', usecols={'age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking',	'time', 'DEATH_EVENT'})
df_heart_failure

df_heart_failure.info()

df_heart_failure.isnull().sum()

# Caso tenha algum valor nulo: df_heart_failure = df_heart_failure.dropna()

df_heart_failure['creatinine_phosphokinase'].value_counts()

df_heart_failure['DEATH_EVENT'].value_counts()

grafio = df_heart_failure['time'].hist(bins=30)

"""### Remoção de Outliers"""

import seaborn as sns
sns.boxplot(df_heart_failure['creatinine_phosphokinase'])

ndf = ''

Q1 = df_heart_failure['creatinine_phosphokinase'].quantile(0.25)
Q3 = df_heart_failure['creatinine_phosphokinase'].quantile(0.75)
IQR = Q3 - Q1
filter = (df_heart_failure['creatinine_phosphokinase'] >= Q1 - 1.5 * IQR) & (df_heart_failure['creatinine_phosphokinase'] <= Q3 + 1.5 *IQR)
ndf = df_heart_failure.loc[filter]
print(ndf.shape)

sns.boxplot(ndf['ejection_fraction'])

Q1 = ndf['ejection_fraction'].quantile(0.25)
Q3 = ndf['ejection_fraction'].quantile(0.75)
IQR = Q3 - Q1
filter = (ndf['ejection_fraction'] >= Q1 - 1.5 * IQR) & (ndf['ejection_fraction'] <= Q3 + 1.5 *IQR)
ndf = ndf.loc[filter]
print(ndf.shape)

sns.boxplot(ndf['platelets'])

Q1 = ndf['platelets'].quantile(0.25)
Q3 = ndf['platelets'].quantile(0.75)
IQR = Q3 - Q1
filter = (ndf['platelets'] >= Q1 - 1.5 * IQR) & (ndf['platelets'] <= Q3 + 1.5 *IQR)
ndf = ndf.loc[filter]
print(ndf.shape)

sns.boxplot(ndf['serum_creatinine'])

Q1 = ndf['serum_creatinine'].quantile(0.25)
Q3 = ndf['serum_creatinine'].quantile(0.75)
IQR = Q3 - Q1
filter = (ndf['serum_creatinine'] >= Q1 - 1.5 * IQR) & (ndf['serum_creatinine'] <= Q3 + 1.5 *IQR)
ndf = ndf.loc[filter]
print(ndf.shape)

sns.boxplot(ndf['serum_sodium'])

Q1 = ndf['serum_sodium'].quantile(0.25)
Q3 = ndf['serum_sodium'].quantile(0.75)
IQR = Q3 - Q1
filter = (ndf['serum_sodium'] >= Q1 - 1.5 * IQR) & (ndf['serum_sodium'] <= Q3 + 1.5 *IQR)
ndf = ndf.loc[filter]
print(ndf.shape)

sns.boxplot(ndf['time'])

ndf

df_heart = ndf

"""### Preparar dados de treino e teste"""

# Separando o X e o Y
features = df_heart.drop(['DEATH_EVENT'], axis=1) # Caso tenha um ID, também é necessário dropa-los aqui
x = features

y = df_heart[['DEATH_EVENT']]

df_heart['DEATH_EVENT'].value_counts()

# Under Sampling
#from imblearn.under_sampling import RandomUnderSampler
#rus = RandomUnderSampler(random_state=0)
#x_resampled, y_resampled = rus.fit_resample(x, y)

# Over Sampling
from imblearn.over_sampling import SMOTE, ADASYN

x_resampled, y_resampled = SMOTE().fit_resample(x, y)

y_resampled['DEATH_EVENT'].value_counts()

# Dividindo dados para treino e dados para teste
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, 
                                                    test_size = 0.2, 
                                                    random_state = 42)

# Instaciando o objeto scaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Fit + transform no conjunto de treino
# Utilizando explicitamente as colunas de ambos os lados força que o
# resultado da normalização ainda seja o dataframe (muito mais facil de manipular) e não um numpy array
x_train[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']] = scaler.fit_transform(x_train[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']])

# Agora utilizando o scaler no conjunto de teste
# Utilizar apenas o transform, pois fit é só no conjunto de treino,]
# o conjunto de teste é utilizado para medir a capacidade de generalização do modelo no mundo real (dados não vistos)
# então faz sentido que a mesma normalização treinada e submetida ao conjunto de treino seja apenas aplicada no de teste
x_test[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']] = scaler.transform(x_test[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']])

"""### Regressão Logistica"""

from sklearn.linear_model import LogisticRegression

modelo_lr = LogisticRegression(solver='lbfgs', max_iter=500)
modelo_lr.fit(x_train, y_train.squeeze())

print('Acertividade treino: ', modelo_lr.score(x_train, y_train))
print('Acertividade teste: ', modelo_lr.score(x_test, y_test.squeeze()))

from sklearn.metrics import recall_score, precision_score, f1_score

y_pred = modelo_lr.predict(x_test)
print( 'Revocação: ', recall_score( y_test, y_pred ))

# Matriz de confusão
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

matriz_confusao = confusion_matrix(y_test, y_pred, normalize = 'true')

display = ConfusionMatrixDisplay(confusion_matrix = matriz_confusao, display_labels = ['Não Morreu', 'Morreu'])

display.plot()
plt.show()

"""###Pycaret"""

!pip install pycaret

train = df_heart.sample(frac=0.9, random_state=786)
test = df_heart.drop(train.index)

from pycaret.utils import enable_colab
enable_colab()

from pycaret.classification import *

s = setup(train, target='DEATH_EVENT', session_id=1, fix_imbalance=True) # Caso a tabela tenha id, usar: ignore_features=['id']

best = compare_models()

evaluate_model(best)

tuned = tune_model(best)

prediction = predict_model(tuned, data=test)

prediction

"""###Funções extras"""

# One hot in code
#df_infos = pd.get_dummies(df_infos, columns=["TRANSPORTE"])

# transformar false e true em 1 e 0
#df_infos.loc[df_infos['AUTO_ACEITE'] == True, 'AUTO_ACEITE'] = 1
#df_infos.loc[df_infos['AUTO_ACEITE'] == False, 'AUTO_ACEITE'] = 0

# Split de elementos
#df[['Date', 'exclude']] = df['ULTIMO_PEDIDO'].str.split('T', expand=True)

# Descobrir valor maximo
# df['Date'].max()